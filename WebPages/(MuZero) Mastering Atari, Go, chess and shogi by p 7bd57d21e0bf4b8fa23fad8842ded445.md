# (MuZero) Mastering Atari, Go, chess and shogi by planning with a learned model

Auteur: DeepMind
Lien: https://sci-hub.hkvisa.net/10.1038/s41586-020-03051-4
Note sur 5: ⭐️⭐️⭐️⭐️
Statut: Terminé
Type: Article

# (MuZero) Mastering Atari, Go, chess and shogi by planning with a learned model

![Untitled]((MuZero)%20Mastering%20Atari,%20Go,%20chess%20and%20shogi%20by%20p%207bd57d21e0bf4b8fa23fad8842ded445/Untitled.png)

# Introduction :

In this paper, they introduice MuZero and what it aims to achieve. In this paper, they say that the limitations of AlphaZero are “*However, these planning
algorithms all rely on knowledge of the environment’s dynamics, such
as the rules of the game or an accurate simulator, preventing their direct
application to real-world domains such as robotics, industrial control
or intelligent assistants, where the dynamics are normally unknown*.”. To solve this, they use a combination of learned model and AlphaZero’s algorithms.

# Idea of MuZero :

## Algorithm short idea :

- Output :
    - ?
- Input :
    - ?
- Algo :
    1. Predict a hidden state (to keep only relevant information)
    2. Update/Improve the hidden state using the previous state and the hypothetical action
    3. A model that predicts the action to play, the value of the state and the reward. The goal of this model is only to predict accurately these three important quantities, to match the improved policy and value function
    generated by search, as well as the observed reward. 

s

## In more details :

1. How MuZero uses its model to plan (note that this is for a MCTS):

$$
board \xrightarrow{h} s^0
$$

$$
(s^{k-1},a^k) \xrightarrow{g} (r^k,s^k)
$$

$$
s^k \xrightarrow{f}(p^k,v^k)
$$

1. How MuZero acts in its environnement
    1. It samples an action, a_{t+1} from the policy \pi_t
    2. From this action, the environnement generates a new observation, o_{t+1}, and a new reward, u_{t+1}
    3. At the end of the episode, the trajectory data are stored into a replay buffer
    

### How MuZero trains its model

1. Sample a trajectory from the replay buffer
2. For each episode t:
    
    a. Give o_1, …, o_t to h
    
    b. For each step k of the MCTS episode k:
    

```
i. Give s^{k-1} and a_{t+k} to g

ii. Give s^{k} to f
```

      c. Update f, g, and h parameters to predict (p^k = \\pi_{t+k}, v^k = z_{k+t}, r^k = u_{k+t})

## Notations :

- ***h*** is the representation function
- ***f*** is the prediction function
- ***g*** is the dynamic function
    
    ![Capture d’écran du 2023-05-19 14-46-46.png]((MuZero)%20Mastering%20Atari,%20Go,%20chess%20and%20shogi%20by%20p%207bd57d21e0bf4b8fa23fad8842ded445/Capture_dcran_du_2023-05-19_14-46-46.png)
    

# MuZero details :

In fact, MuZero uses only ONE model to predict 

$$
the\ policy : p_t^k = \pi(a_{t+k+1}|o_1, \cdots,o_t, a_{t+1},\cdots,a_{t+k})
$$

$$
the\ value : v_t^k = \mathbb{E}[u_{t+k+1 + \gamma u_{t+k+2} + \cdots} | |o_1, \cdots,o_t, a_{t+1},\cdots,a_{t+k}]

$$

$$
the\ immediate\ reward : r_t^k = u_{t+k}
$$

With *u* the true observed reward, π is the policy used to select an action and γ the discounted factor

Then, from these information, they use the following algorithm (TODO : Compared it with AlphaZero’s MCTS search algo) :

$$
Selection \rightarrow Expansion \rightarrow Backup
$$

The loss of the model is the following one :

$$
l_t(\theta) = \sum_{k=0}^{K}l^p(\pi_{t+k},p_t^k) + \sum_{k=0}^{K}l^v(z_{t+k},v_t^k) + \sum_{k=1}^{K}l^r(u_{t+k},r_t^k) + c||\theta||²
$$

# Questions :

1. How different is their hidden state prediction of World model ?
2. We have details about the MuZero, but not on the hidden states 
3. What do they mean by “trajectory data” ? All of the MCTS
4. What does “the dynamics function is represented deterministically” mean ? 
5. What is the representation function ? Because what does “by encoding past observations” mean ? It takes only observations as parameters but it uses the notation of Theta of the model parameters